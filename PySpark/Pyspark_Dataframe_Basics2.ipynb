{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzRQt4ljgVub",
        "outputId": "65886932-c147-438f-c2a8-8e90d6c18ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=b94082ec4495e87d76676ee5f633dba6dbceb3437b4900b74be13085ec77eeda\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "6ypkKWaggpMf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark= SparkSession.builder.appName(\"test\").getOrCreate()"
      ],
      "metadata": {
        "id": "nZqn-5NNg0of"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'rohith'),(2,'Mohith')]\n",
        "df= spark.createDataFrame(data= data,schema=[\"id\",\"name\"])"
      ],
      "metadata": {
        "id": "khSgQ6MfhVFu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M7ApaLmh1w5",
        "outputId": "828eab39-5418-4f6e-b6ce-c3b34a80c095"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|rohith|\n",
            "|  2|Mohith|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjZaKEY3h8H1",
        "outputId": "c0003a83-25a5-4889-b6d7-54901964f98e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.createDataFrame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-onOl7FiOSx",
        "outputId": "710346dd-9db3-40ab-937d-01cfadfc92be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method createDataFrame in module pyspark.sql.session:\n",
            "\n",
            "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
            "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
            "    or a :class:`numpy.ndarray`.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    data : :class:`RDD` or iterable\n",
            "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
            "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
            "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
            "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
            "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            "        column names, default is None. The data type string format equals to\n",
            "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            "        omit the ``struct<>``.\n",
            "    \n",
            "        When ``schema`` is a list of column names, the type of each column\n",
            "        will be inferred from ``data``.\n",
            "    \n",
            "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            "        from ``data``, which should be an RDD of either :class:`Row`,\n",
            "        :class:`namedtuple`, or :class:`dict`.\n",
            "    \n",
            "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
            "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
            "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
            "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
            "        later.\n",
            "    samplingRatio : float, optional\n",
            "        the sample ratio of rows used for inferring. The first few rows will be used\n",
            "        if ``samplingRatio`` is ``None``.\n",
            "    verifySchema : bool, optional\n",
            "        verify data types of every row against schema. Enabled by default.\n",
            "    \n",
            "        .. versionadded:: 2.1.0\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Create a DataFrame from a list of tuples.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
            "    +-----+---+\n",
            "    |   _1| _2|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a list of dictionaries.\n",
            "    \n",
            "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
            "    >>> spark.createDataFrame(d).show()\n",
            "    +---+-----+\n",
            "    |age| name|\n",
            "    +---+-----+\n",
            "    |  1|Alice|\n",
            "    +---+-----+\n",
            "    \n",
            "    Create a DataFrame with column names specified.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the explicit schema specified.\n",
            "    \n",
            "    >>> from pyspark.sql.types import *\n",
            "    >>> schema = StructType([\n",
            "    ...    StructField(\"name\", StringType(), True),\n",
            "    ...    StructField(\"age\", IntegerType(), True)])\n",
            "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the schema in DDL formatted string.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create an empty DataFrame.\n",
            "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
            "    as the DataFrame lacks data from which the schema can be inferred.\n",
            "    \n",
            "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
            "    +----+---+\n",
            "    |name|age|\n",
            "    +----+---+\n",
            "    +----+---+\n",
            "    \n",
            "    Create a DataFrame from Row objects.\n",
            "    \n",
            "    >>> from pyspark.sql import Row\n",
            "    >>> Person = Row('name', 'age')\n",
            "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
            "    >>> df.show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a pandas DataFrame.\n",
            "    \n",
            "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            "    +---+---+\n",
            "    |  0|  1|\n",
            "    +---+---+\n",
            "    |  1|  2|\n",
            "    +---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "89FcKDkKiWEp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(StructType)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysCQqumMi6Uq",
        "outputId": "fc28d1ce-7b9f-4da5-f675-abc77ace9a0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class StructType in module pyspark.sql.types:\n",
            "\n",
            "class StructType(DataType)\n",
            " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |  \n",
            " |  Struct type, consisting of a list of :class:`StructField`.\n",
            " |  \n",
            " |  This is the data type representing a :class:`Row`.\n",
            " |  \n",
            " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
            " |  A contained :class:`StructField` can be accessed by its name or position.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import *\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1[\"f1\"]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  >>> struct1[0]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  \n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
            " |  >>> struct1 == struct2\n",
            " |  False\n",
            " |  \n",
            " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
            " |  using class:`StructType` and class:`StructField`:\n",
            " |  \n",
            " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
            " |  >>> schema = StructType([\n",
            " |  ...     StructField(\"name\", StringType()),\n",
            " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
            " |  ... ])\n",
            " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
            " |  >>> df.printSchema()\n",
            " |  root\n",
            " |   |-- name: string (nullable = true)\n",
            " |   |-- languagesSkills: array (nullable = true)\n",
            " |   |    |-- element: string (containsNull = true)\n",
            " |  >>> df.show()\n",
            " |  +-----+---------------+\n",
            " |  | name|languagesSkills|\n",
            " |  +-----+---------------+\n",
            " |  |Alice|  [Java, Scala]|\n",
            " |  |  Bob|[Python, Scala]|\n",
            " |  +-----+---------------+\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructType\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
            " |      Access fields by name or slice.\n",
            " |  \n",
            " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
            " |      Iterate the fields\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Return the number of fields.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
            " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
            " |      The method accepts either:\n",
            " |      \n",
            " |          a) A single parameter which is a :class:`StructField` object.\n",
            " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
            " |             metadata(optional). The data_type parameter may be either a String or a\n",
            " |             :class:`DataType` object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field : str or :class:`StructField`\n",
            " |          Either the name of the field or a :class:`StructField` object\n",
            " |      data_type : :class:`DataType`, optional\n",
            " |          If present, the DataType of the :class:`StructField` to create\n",
            " |      nullable : bool, optional\n",
            " |          Whether the field to add should be nullable (default True)\n",
            " |      metadata : dict, optional\n",
            " |          Any additional metadata (default None)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
            " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |  \n",
            " |  fieldNames(self) -> List[str]\n",
            " |      Returns all field names in a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
            " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct.fieldNames()\n",
            " |      ['f1']\n",
            " |  \n",
            " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: Tuple) -> Tuple\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
            " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
            " |      \n",
            " |      Below is a JSON schema it must adhere to::\n",
            " |      \n",
            " |          {\n",
            " |            \"title\":\"StructType\",\n",
            " |            \"description\":\"Schema of StructType in json format\",\n",
            " |            \"type\":\"object\",\n",
            " |            \"properties\":{\n",
            " |               \"fields\":{\n",
            " |                  \"description\":\"Array of struct fields\",\n",
            " |                  \"type\":\"array\",\n",
            " |                  \"items\":{\n",
            " |                      \"type\":\"object\",\n",
            " |                      \"properties\":{\n",
            " |                         \"name\":{\n",
            " |                            \"description\":\"Name of the field\",\n",
            " |                            \"type\":\"string\"\n",
            " |                         },\n",
            " |                         \"type\":{\n",
            " |                            \"description\": \"Type of the field. Can either be\n",
            " |                                            another nested StructType or primitive type\",\n",
            " |                            \"type\":\"object/string\"\n",
            " |                         },\n",
            " |                         \"nullable\":{\n",
            " |                            \"description\":\"If nulls are allowed\",\n",
            " |                            \"type\":\"boolean\"\n",
            " |                         },\n",
            " |                         \"metadata\":{\n",
            " |                            \"description\":\"Additional metadata to supply\",\n",
            " |                            \"type\":\"object\"\n",
            " |                         },\n",
            " |                         \"required\":[\n",
            " |                            \"name\",\n",
            " |                            \"type\",\n",
            " |                            \"nullable\",\n",
            " |                            \"metadata\"\n",
            " |                         ]\n",
            " |                      }\n",
            " |                 }\n",
            " |              }\n",
            " |           }\n",
            " |         }\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      json : dict or a dict-like object e.g. JSON object\n",
            " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
            " |          each of which must have specific keys (name, type, nullable, metadata).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> json_str = '''\n",
            " |      ...  {\n",
            " |      ...      \"fields\": [\n",
            " |      ...          {\n",
            " |      ...              \"metadata\": {},\n",
            " |      ...              \"name\": \"Person\",\n",
            " |      ...              \"nullable\": true,\n",
            " |      ...              \"type\": {\n",
            " |      ...                  \"fields\": [\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"name\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      },\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"surname\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      }\n",
            " |      ...                  ],\n",
            " |      ...                  \"type\": \"struct\"\n",
            " |      ...              }\n",
            " |      ...          }\n",
            " |      ...      ],\n",
            " |      ...      \"type\": \"struct\"\n",
            " |      ...  }\n",
            " |      ...  '''\n",
            " |      >>> import json\n",
            " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
            " |      >>> scheme.simpleString()\n",
            " |      'struct<Person:struct<name:string,surname:string>>'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from DataType:\n",
            " |  \n",
            " |  typeName() -> str from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Schema=StructType([StructField(\"id\", IntegerType()),\n",
        "                   StructField(\"name\", StringType())])"
      ],
      "metadata": {
        "id": "PkRO8b4gjLGX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= spark.createDataFrame(data =data, schema=Schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HROLnXLFkb-3",
        "outputId": "90a4dfa7-f0ee-4c18-cf92-ef5f5784cbc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|rohith|\n",
            "|  2|Mohith|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY1G9i-ik26j",
        "outputId": "bf568131-89ab-446c-c630-0ecf9fb2e61c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EEKJKGLlBGn",
        "outputId": "fb117cb4-b91c-46f0-c6ca-44b19d4aa127"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'), ('name', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'id':1,'name':'rohith'},{'id':2,'name':'Mohith'}]\n",
        "df= spark.createDataFrame(data = data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k4PAJrflH27",
        "outputId": "72df9a4d-fe3a-4515-b0d8-3bb17efde5e3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|rohith|\n",
            "|  2|Mohith|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f7RzoIemIm6",
        "outputId": "1b097d6f-d830-4fdf-b376-12fa252991b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "H5USqIM_mS3g",
        "outputId": "d893f9fb-4849-4099-fa11-b79c72dfb3d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tJJcN8Gum9N0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}